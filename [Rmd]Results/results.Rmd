---
title: "Result notes"
author: "Yuta Suzuki"
date: "3/2/2021"
output: 
  output: word_document
  # html_document
  # :
  #     reference_docx: results.docx
  # html_document
draft: yes
# 
---
<style type="text/css">
body{
font-size: 14pt;
line-height: 2;
width: 100%;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# tinytex::install_tinytex()
```

```{r child="readFunc.Rmd"}
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}

mmFlg = TRUE
auFlg = FALSE

if(mmFlg){
  load("./data/Exp1/figure2_mm.rda")
}else if(auFlg){
  load("./data/Exp1/figure2_au.rda")
}else{
  load("./data/Exp1/figure2.rda")
}

numOfsub = length(unique(data_res_tonic$sub))

#### Behavioral(# of trials) ####
ave_numOfTrial = aggregate( numOfTrial ~ numOfSwitch, data = data_numOfTrial, FUN = "mean")
ave_numOfTrial$numOfTrial = round(ave_numOfTrial$numOfTrial,2)
sd_numOfTrial = aggregate( numOfTrial ~ numOfSwitch, data = data_numOfTrial, FUN = "sd")
sd_numOfTrial$numOfTrial = round(sd_numOfTrial$numOfTrial,2)
anovakun(data_numOfTrial,"sA",long=T, peta=T,gg=T)

data_numOfTrial$numOfSwitch = factor(data_numOfTrial$numOfSwitch,levels = unique(data_numOfTrial$numOfSwitch))
data_numOfTrial$sub = factor(data_numOfTrial$sub,levels = unique(data_numOfTrial$sub))

numOfTrial_BF = anovaBF(numOfTrial ~ numOfSwitch + sub, data=data_numOfTrial, whichRandom = "sub",progress=FALSE)
numOfTrial_BF = round(exp(numOfTrial_BF@bayesFactor[["bf"]]),3)

numOfTrial_table = forDrawingSigANOVA
numOfTrial_post = forDrawingPost[["A"]][["bontab"]]

#### Behavioral(number of switch and jitter) ####
saveLoc = "../[Python]PreProcessing/Exp1/data/"
dat_jitter=fromJSON(file=paste(saveLoc,"numOfSwitch_jitter.json", sep = ""))
 
ind_jitter <- data.frame(
  sub = unlist(dat_jitter$sub),
  numOfSwitch = unlist(dat_jitter$numOfSwitch),
  jitter = unlist(dat_jitter$taskTimeLen)
)
dat_jitter = aggregate( . ~ sub*numOfSwitch, data = ind_jitter, FUN = "mean")
anovakun(dat_jitter,"sA",long=T, peta=T,gg=T)

jitter_table_e1 = forDrawingSigANOVA

dat_jitter$numOfSwitch = factor(dat_jitter$numOfSwitch,levels = unique(dat_jitter$numOfSwitch))
dat_jitter$sub = factor(dat_jitter$sub,levels = unique(dat_jitter$sub))

dat_jitter_BF_e1 = anovaBF(jitter ~ numOfSwitch + sub, data=dat_jitter, whichRandom = "sub",progress=FALSE)
dat_jitter_BF_e1 = round(exp(dat_jitter_BF_e1@bayesFactor[["bf"]]),3)


#### Behavioral(RT) #### 
anovakun(data_RT,"sA",long=T, peta=T,gg=T)
rt_sd = aggregate( RT ~ numOfSwitch, data = data_RT, FUN = "sd")
rt_ave = aggregate( RT ~ numOfSwitch, data = data_RT, FUN = "mean")
rt_sd$RT = round(rt_sd$RT,3)
rt_ave$RT = round(rt_ave$RT,3)

RT_table = forDrawingSigANOVA

data_RT$numOfSwitch = factor(data_RT$numOfSwitch,levels = unique(data_RT$numOfSwitch))
data_RT$sub = factor(data_RT$sub,levels = unique(data_RT$sub))

data_RT_BF = anovaBF(RT ~ numOfSwitch + sub, data=data_RT, whichRandom = "sub",progress=FALSE)
data_RT_BF = round(exp(data_RT_BF@bayesFactor[["bf"]]),3)

#### Figure 2A(average baseline pupil size) ####
data_res_tonic$data_y = data_res_tonic$Size
data_res_tonic$Size = NULL
anovakun(data_res_tonic,"sA",long=T, peta=T,gg=T)

data_res_tonic$numOfSwitch = factor(data_res_tonic$numOfSwitch,levels = unique(data_res_tonic$numOfSwitch))
data_res_tonic$sub = factor(data_res_tonic$sub,levels = unique(data_res_tonic$sub))

resultByesAnova = anovaBF(data_y ~ numOfSwitch + sub, data=data_res_tonic, whichRandom = "sub",progress=FALSE)
resultByesAnova = round(exp(resultByesAnova@bayesFactor[["bf"]]),3)

table_fig2 = forDrawingSigANOVA
# tVal_fig2 = forDrawingPost[["A"]][["bontab"]][["t"]]
# pVal_fig2 = forDrawingPost[["A"]][["bontab"]][["adj.p"]]
BP_e1_post = forDrawingPost[["A"]][["bontab"]]


x = data_res_tonic[data_res_tonic$numOfSwitch == "2+",]$data_y
y = data_res_tonic[data_res_tonic$numOfSwitch == "0",]$data_y
n = length(x)
sc = sqrt((n*(var(x))+n*(var(y)))/(n*2))
cohen_d0 = round(abs(mean(x)-mean(y))/sc,3)

bayesF = ttestBF(x = x, y = y, paired=TRUE)
bayesF_d0 = round(exp(bayesF@bayesFactor[["bf"]]),3)

x = data_res_tonic[data_res_tonic$numOfSwitch == "2+",]$data_y
y = data_res_tonic[data_res_tonic$numOfSwitch == "1",]$data_y
n = length(x)
sc = sqrt((n*(var(x))+n*(var(y)))/(n*2))
cohen_d1 = round(abs(mean(x)-mean(y))/sc,3)
bayesF = ttestBF(x = x, y = y, paired=TRUE)
bayesF_d1 = round(exp(bayesF@bayesFactor[["bf"]]),3)

#### Figure 2B (tertile) #####
if(mmFlg){
  data = fromJSON(file="../[Python]PreProcessing/Exp1/data/data_tertile_mm.json")
}else if(auFlg){
  data = fromJSON(file="../[Python]PreProcessing/Exp1/data/data_tertile_au.json")
}else{
  data = fromJSON(file="../[Python]PreProcessing/Exp1/data/data_tertile.json")
}

data_tertile = data.frame(
  sub = data$sub,
  data_y = data$numOfSwitch_sorted,
  Tertile = data$tertile
)

# data_tertile = data_tertile_e2
x = as.numeric(data_tertile$Tertile)
data_tertile$Tertile = as.numeric(data_tertile$Tertile)

y = data_tertile$data_y
f <- y ~  a*x + b
model1 <- nls(f, start = c(a = 0, b = 0))

f <- y ~  a*x^2 + b*x + c
model2 <- nls(f, start = c(a = 0, b = 0, c=0))

if (AIC(model1) > AIC(model2)){
  model = model2
}else{
  model = model1
}
df <- data.frame(x = seq(1, 5, length = 10))
data_curve = data.frame(
  Tertile = df$x,
  yy = predict(model, df)
)

tertile_e1_tVal = round(summary(model)[["coefficients"]]['b',]['t value'],4)
tertile_e1_pVal = round(summary(model)[["coefficients"]]['b',]['Pr(>|t|)'],4)

tertile_e1_BF = regressionBF(data_y ~ Tertile, data = data_tertile)
tertile_e1_BF = round(exp(tertile_e1_BF@bayesFactor[["bf"]]),3)

model_fig2b = model
data_tertile = aggregate( . ~ sub*Tertile, data = data_tertile, FUN = "mean")
# lm_model = lm(data = data_tertile, data_y ~ Tertile)
r = cor.test(x,y)

regBF_e1 = regressionBF(data_y ~ Tertile, data = data_tertile, progress=FALSE)
regBF_e1 = round(exp(regBF_e1@bayesFactor[["bf"]]),3)

#### Figure 2B (heatmap) #####
data=fromJSON(file="../[Python]PreProcessing/Exp1/heatmap/corr2.json")
melted_data <- melt(data)
p_values=fromJSON(file="../[Python]PreProcessing/Exp1/heatmap/p_values2.json")
melted_p_values <- melt(p_values)
melted_data$p.value = round(melted_p_values$value,3)
melted_data$value = round(melted_data$value,3)

#### Figure 3A (cross-corr) #####
data = fromJSON(file="../[Python]PreProcessing/Exp1/data/data_cross_corr.json")

dat <- list(matrix(unlist(data$raw),nrow=length(data$raw),byrow=T),
            matrix(unlist(data$raw_queue),nrow=length(data$raw_queue),byrow=T))
names(dat) <- c('trial','seconds')
ggName =  c('shuffled','raw')
numOfTrial = dim(dat$trial)[1]
numOfSub = length(unique(data$sub))
lengthOfTime = dim(dat$trial)[2]

ind_data <- data.frame(
  sub =  rep( data$sub, times = rep( lengthOfTime, numOfTrial)),
  data_y = c(t(matrix(t(dat$trial),nrow=1))),
  condition = rep(ggName[data$randFlag+1], times = rep( lengthOfTime, numOfTrial)),
  data_x = data$lags_trial
)

data_ccorr = aggregate( data_y ~ sub*condition*data_x, data = ind_data, FUN = "mean")
data_ccorr = data_ccorr[order(data_ccorr$sub,data_ccorr$condition),]
t = aggregate( data_y ~ sub*condition, data = data_ccorr, FUN = "max")
maxLag = NULL
for(i in 1 : length(t$data_y)){
  tmp = data.frame(
    sub = t$sub[i],
    condition = t$condition[i],
    data_y = data_ccorr[data_ccorr$data_y == t$data_y[i],]$data_x
  )
  maxLag = rbind(maxLag,tmp)
}

#### Figure 3B (peak corr.) #####
maxVal = aggregate( data_y ~ sub*condition, data = data_ccorr, FUN = "max")
x = maxVal[maxVal$condition == 'raw',]$data_y
y = maxVal[maxVal$condition == 'shuffled',]$data_y
n = length(x)
sc = sqrt((n*(var(x))+n*(var(y)))/(n*2))
CCF_max_cohen_d = round(abs(mean(x)-mean(y))/sc,3)
CCF_max = t.test(x,y,var.equal=T,paired=T)
CCF_max_bayesF = ttestBF(x = x, y = y, paired=TRUE)
CCF_max_bayesF = round(exp(CCF_max_bayesF@bayesFactor[["bf"]]),3)

#### Figure 3C (lag at peak corr.) #####
x = maxLag[maxLag$condition == 'raw',]$data_y
CCF_lag_bayesF = ttestBF(x = x)
CCF_lag_bayesF = round(exp(CCF_lag_bayesF@bayesFactor[["bf"]]),3)
CCF_lag = t.test(x, mu=0, alternative="less")

#### Figure 3D (LMM) ####
# load("./data/Exp1/betaWtFunc.rda")
# data=fromJSON(file="./data/Exp1/data_include_past2.json")
# 
# pVal = NULL
# betaWtFunc = data.frame()
# tmpAIC_model2 = NULL
# tmpAIC_model1 = NULL
# for(i in -20:20){
#   ind_data <- data.frame(
#     sub =  unlist(data$sub),
#     PDR = unlist(data[[paste("PDR_lag",i,sep="")]]),
#     ave = unlist(data[["ave"]]),
#     Baseline = unlist(data[["Baseline"]]),
#     diff = unlist(data[["diff"]]),
#     numOfSwitch = unlist(data[["numOfSwitch"]])
#   )
# 
#   ind_data[ind_data$numOfSwitch > 2,]$numOfSwitch = 2
#   ind_data = ind_data[ind_data$PDR != 0,]
# 
#   model = lmer( numOfSwitch ~ PDR + (1+PDR|sub),ind_data)
#   gt = ranef(model)[["sub"]][["PDR"]]
# 
#   model = summary(model)
# 
#   pVal = rbind(pVal,model[["coefficients"]]['PDR',]['Pr(>|t|)'])
# 
#   subpVal=NULL
#   for(iSub in unique(unlist(data$sub))){
#     tmp  = ind_data[ind_data$sub == iSub,]
#     submodel = lm(numOfSwitch ~ PDR, data = tmp)
#     subpVal = rbind(subpVal,summary(submodel)[["coefficients"]]['PDR',]['Pr(>|t|)'])
#   }
# 
#   betaWt = data.frame(
#     sub = unique(unlist(data$sub)),
#     data_y = gt + model[["coefficients"]]['PDR',]['Estimate'],
#     lag = i,
#     pval = p.adjust(as.numeric(subpVal), method = "BH")
#   )
#   betaWtFunc =  rbind(betaWtFunc,betaWt)
# 
# }
# # save(betaWtFunc, pVal, file = "./data/Exp1/betaWtFunc.rda")
# 
# betaWtFunc$sub = subName[betaWtFunc$sub]
# dat_ave = aggregate( data_y ~ lag, data = betaWtFunc, FUN = "mean")
# dat_ave$sub = 'average'
# dat_ave$pval = p.adjust(pVal, method = "BH")
# res_lmm = dat_ave[dat_ave['pval'] < 0.05,]$lag
# 
# #### Figure 3E (AUC) #### 
# data=fromJSON(file="./data/Exp1/roc_past2.json")
# 
# ggName = c('Baseline','Transient')
# ggName2 = c('e1','e2')
# dat <- list((matrix(unlist(data$FalsePos),nrow=length(data$FalsePos),byrow=T)),
#             (matrix(unlist(data$TruePos),nrow=length(data$TruePos),byrow=T)))
# 
# names(dat) <- c('FalsePos','TruePos')
# 
# numOfSub = length(unique(data$sub))
# numOfTrial = dim(dat$FalsePos)[1]
# lengthOfTime = dim(dat$FalsePos)[2]
# timeLen = c(0,1)
# 
# ind_data_ROC = data.frame(
#   sub =  rep( data$sub, times = rep( lengthOfTime, numOfTrial)),
#   data_y = t(matrix(t(dat$TruePos),nrow=1)),
#   data_x = t(matrix(t(dat$FalsePos),nrow=1)),
#   lag = rep( as.character(data$lag), times = rep( lengthOfTime, numOfTrial)),
#   condition = rep( ggName[unlist(data$type)+1], times = rep( lengthOfTime, numOfTrial)),
#   exp = rep( ggName2[unlist(data$exp)], times = rep( lengthOfTime, numOfTrial))
# )
# ind_data_ROC$lag <- factor(ind_data_ROC$lag,levels =c("-20","-15","-10","-5","0","5","10","15","20"))
# ind_data_ROC = ind_data_ROC[ind_data_ROC$exp == 'e1' & ind_data_ROC$condition == 'Baseline' ,]
# ind_data_ROC = aggregate( . ~ lag, data = ind_data_ROC, FUN = "mean")
# 
# p = ggplot(ind_data_ROC,aes(x=data_y,y = data_x,group = interaction(sub,lag),color=lag))+
#   geom_line()+
#   annotate(x=0, xend=1, y=0, yend=1, colour="black", lwd=0.5, geom="segment")+
#   facet_wrap(sub ~ .)
# 
# 
# ind_data = data.frame(
#   sub = unlist(data$sub),
#   data_y =unlist( data$AUC),
#   lag = unlist(data$lag),
#   condition = ggName[unlist(data$type)+1],
#   exp = ggName2[unlist(data$exp)]
# )
# 
# dat_p = data.frame()
# for(iExp in 1:2){
#   
#   for (iCondition in 1:2){
#     pVal = NULL
#     for(iLag in unique(ind_data$lag)){
#       tmp = ind_data[ind_data$lag == iLag & 
#                        ind_data$exp == ggName2[iExp] &
#                        ind_data$condition == ggName[iCondition],]
#       d = t.test(tmp$data_y,mu=0.5, alternative="greater")
#       pVal = rbind(pVal,round(d[["p.value"]], digits = 4))
#     }
#     tmp_dat_p = data.frame(
#       pVal = p.adjust(pVal, method = "BH"),
#       data_y = 0,
#       data_x = unique(ind_data$lag),
#       condition = ggName[iCondition],
#       exp = ggName2[iExp]
#     )
#     dat_p = rbind(dat_p,tmp_dat_p)
#   }
# }
# 
# dat_p['flag'] = rep('gray',dim(dat_p)[1])
# ind = dat_p['pVal'] < 0.1 & dat_p['pVal'] > 0.05 & dat_p$condition == 'Baseline'
# dat_p['data_y'][ind] = 0.58
# dat_p['flag'][ind] = 'gray'
# 
# ind = dat_p['pVal'] < 0.1 & dat_p['pVal'] > 0.05 & dat_p$condition == 'Transient'
# dat_p['data_y'][ind] = 0.44
# dat_p['flag'][ind] = 'gray'
# 
# ind = dat_p['pVal'] < 0.05 & dat_p$condition == 'Baseline'
# dat_p['data_y'][ind] = 0.58
# dat_p['flag'][ind] = 'black'
# 
# ind = dat_p['pVal'] < 0.05 & dat_p$condition == 'Transient'
# dat_p['data_y'][ind] = 0.44
# dat_p['flag'][ind] = 'black'
# 
# ind_data_anova = ind_data[ind_data$exp == 'e1',]
# ind_data_anova$exp = NULL
# ind_data_anova = ind_data_anova[,c(1,3,4,2)]

```

# Results
## Experiment 1
### Behavioral response
Participants reported the number of perceptual alternations from 0 to 5. Since the trial numbers were unbalanced among these responses (__Fig. 1C__), we classified the number of alternation responses into 0, 1, and more than 2 times (hereafter, referred to as 0-, 1-, >1-alt cases, respectively). The average numbers of trials were 
`r ave_numOfTrial[ave_numOfTrial$numOfSwitch == '0',]$numOfTrial` $\pm$ `r sd_numOfTrial[sd_numOfTrial$numOfSwitch == '0',]$numOfTrial`, 
`r ave_numOfTrial[ave_numOfTrial$numOfSwitch == '1',]$numOfTrial` $\pm$ `r sd_numOfTrial[sd_numOfTrial$numOfSwitch == '1',]$numOfTrial`, and 
`r ave_numOfTrial[ave_numOfTrial$numOfSwitch == '2+',]$numOfTrial` $\pm$ `r sd_numOfTrial[sd_numOfTrial$numOfSwitch == '2+',]$numOfTrial`, respectively 
($F$(`r round(numOfTrial_table[2,]['df.col'],2)`,`r round(numOfTrial_table[3,]['df.col'],2)`) = `r round(numOfTrial_table[2,]['f.col'],3)`, 
$p$ = `r round(numOfTrial_table[2,]['p.col'],3)`, $\eta^2_p$ = `r round(numOfTrial_table[2,]['p.eta^2'],3)`, $BF_{10}$ = `r numOfTrial_BF`). 
Post analysis showed that the number of trials for 1-alt cases was significantly larger than for 0- and >1-alt cases (
$t$(1, `r numOfTrial_post[2,]$df`) = `r round(numOfTrial_post[2,]$t,3)`, $p$ = `r round(numOfTrial_post[2,]$p.value, 3)`, 
$t$(1, `r numOfTrial_post[1,]$df`) = `r round(numOfTrial_post[1,]$t,3)`, $p$ = `r round(numOfTrial_post[1,]$p.value, 3)`) 
at alpha level of 0.05/3 corrected by a Bonferroni-Holm method.
RTs of each response category were 
`r rt_ave[rt_ave$numOfSwitch == 0,]$RT` $\pm$ `r rt_sd[rt_sd$numOfSwitch == 0,]$RT` sec, 
`r rt_ave[rt_ave$numOfSwitch == 1,]$RT` $\pm$ `r rt_sd[rt_sd$numOfSwitch == 1,]$RT` sec, and 
`r rt_ave[rt_ave$numOfSwitch == 2,]$RT` $\pm$ `r rt_sd[rt_sd$numOfSwitch == 2,]$RT` sec 
($F$(`r round(RT_table[2,]['df.col'],2)`,`r round(RT_table[3,]['df.col'],2)`) = `r round(RT_table[2,]['f.col'],3)`, 
$p$ = `r round(RT_table[2,]['p.col'],3)`, $\eta^2_p$ = `r round(RT_table[2,]['p.eta^2'],3)`, $BF_{10}$ = `r data_RT_BF`).
Although the observation period was jittered from 5-9 second, there was no statistical differences between the number of perceptual alternations and the observation time 
($F$(`r round(jitter_table_e1[2,]['df.col'],2)`,`r round(jitter_table_e1[3,]['df.col'],2)`) = `r round(jitter_table_e1[2,]['f.col'],3)`, 
$p$ = `r round(jitter_table_e1[2,]['p.col'],3)`, $\eta^2_p$ = `r round(jitter_table_e1[2,]['p.eta^2'],3)`, $BF_{10}$ = `r dat_jitter_BF_e1`). 


### Baseline pupil size
__Figure 2A__ illustrates the grand-averaged baseline pupil changes across participants before the response cue onset, as a function of perceptual alternations number. The one-way repeated measures ANOVA revealed a significant main effect on the number of perceptual alternations 
($F$(`r round(table_fig2[2,]['df.col'],2)`,`r round(table_fig2[3,]['df.col'],2)`) = `r round(table_fig2[2,]['f.col'],3)`, 
$p$ = `r round(table_fig2[2,]['p.col'],3)`, $\eta^2_p$ = `r round(table_fig2[2,]['p.eta^2'],3)`, $BF_{10}$ = `r resultByesAnova`). 
The post-hoc multiple comparisons showed that the baseline pupil size in the >1-alt case was significantly larger than in the 0- and 1-alt cases 
($t$(1, `r numOfsub-1`) = `r round(BP_e1_post[1,]$t,3)`, $p$ = `r round(BP_e1_post[1,]$p.value,3)`, 
Cohen’s  $d_z$ = `r cohen_d0`, $BF_{10}$ =  `r bayesF_d0`; 
$t$(1, `r numOfsub-1`) = `r round(BP_e1_post[2,]$t,3)`, $p$ =  `r round(BP_e1_post[2,]$p.value,3)`, 
Cohen’s  $d_z$ = `r cohen_d1`, $BF_{10}$ =  `r bayesF_d1`, respectively), 
indicating that the baseline pupil size, prior to counting the number of alternations, was related to a subsequent number of perceptual transitions.
The answered number of trials was significantly different among the 0-, 1-, and >1 cases as shown above. Such unbalanced trial numbers can cause biases in the statistical analysis and decreased statistical power. Thus, we performed an alternative analysis to avoid this potential statistical problem. We segregated the trials equally into five bins based on the rank order of baseline pupil size. Results are shown in Fig. 2B. The data were fitted by a simple regression model 
($y$ = `r round(summary(model_fig2b)[['coefficients']]['a',]['Estimate'],3)`$x$ + `r round(summary(model_fig2b)[['coefficients']]['b',]['Estimate'],3)`, 
$R$ = `r round(r[['estimate']][['cor']],3)`, $t$ = `r tertile_e1_tVal`, $p$ = `r tertile_e1_pVal`). 
Consistent with the previous results, the number of alternations monotonically increased with the baseline pupil size.


```{r, message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}

go1 = c("unswitch","switch")

if(mmFlg){
  load("./data/Exp2/Figure3_mm.rda")
}else if(auFlg){
  load("./data/Exp2/figure3_au.rda")
}else{
  load("./data/Exp2/Figure3.rda")
}

#### Behavioral(RT) ####
# anovakun(data_RT,"sA",long=T, peta=T)
rt_sd = aggregate( RT ~ Responses, data = data_RT, FUN = "sd")
rt_ave = aggregate( RT ~ Responses, data = data_RT, FUN = "mean")
rt_sd$RT = round(rt_sd$RT,3)
rt_ave$RT = round(rt_ave$RT,3)

# RT_table = forDrawingSigANOVA
x = data_RT[data_RT$Responses == 0,]$RT
y = data_RT[data_RT$Responses == 1,]$RT

RT_table = t.test(x,y,var.equal=T,paired=T)

RT_cohen_d = cohen.d(x,y,paired=TRUE, within=TRUE)
RT_cohen_d = round(abs(RT_cohen_d[["estimate"]]),3)

# bayesF = ttestBF(x = x, y = y, paired=TRUE)
# bayesF = round(exp(bayesF@bayesFactor[["bf"]]),3)

#### Behavioral(number of switch and jitter) ####
saveLoc = "../[Python]PreProcessing/Exp2/data/"
dat_jitter=fromJSON(file=paste(saveLoc,"numOfSwitch_jitter.json", sep = ""))
 
ind_jitter <- data.frame(
  sub = unlist(dat_jitter$sub),
  Responses = unlist(dat_jitter$responses),
  jitter = unlist(dat_jitter$taskTimeLen)
)
dat_jitter = aggregate( . ~ sub*Responses, data = ind_jitter, FUN = "mean")

x = dat_jitter[dat_jitter$Responses == 0,]$jitter
y = dat_jitter[dat_jitter$Responses == 1,]$jitter

jitter_table_e2 = t.test(x,y,var.equal=T,paired=T)

jitter_cohen_d_e2 = cohen.d(x,y,paired=TRUE, within=TRUE)
jitter_cohen_d_e2 = round(abs(jitter_cohen_d_e2[["estimate"]]),3)

#### Behavioral(trials) ####
data_numOfTrial = aggregate( numOfTrial ~ sub*Responses, data = data_corr, FUN = "max")

ave_numOfTrial = aggregate( numOfTrial ~ Responses, data = data_numOfTrial, FUN = "mean")
ave_numOfTrial$numOfTrial = round(ave_numOfTrial$numOfTrial,2)
sd_numOfTrial = aggregate( numOfTrial ~ Responses, data = data_numOfTrial, FUN = "sd")
sd_numOfTrial$numOfTrial = round(sd_numOfTrial$numOfTrial,2)

x = data_numOfTrial[data_numOfTrial$Responses == 0,]$numOfTrial
y = data_numOfTrial[data_numOfTrial$Responses == 1,]$numOfTrial

numOfTrial_e2_table = t.test(x,y,var.equal=T,paired=T)

numOfTrial_e2_cohen_d = cohen.d(x,y,paired=TRUE, within=TRUE)
numOfTrial_e2_cohen_d = round(abs(numOfTrial_e2_cohen_d[["estimate"]]),3)

numOfTrial_e2_BF = ttestBF(x = x, y = y, paired=TRUE)
numOfTrial_e2_BF = round(exp(numOfTrial_e2_BF@bayesFactor[["bf"]]),3)

#### Figure 4A(# of switch) ####
numOfSub_e2 = length(unique(data_res$sub))

data_res$Responses = go1[data_res$Responses+1]
data_res$Responses = factor(data_res$Responses,levels = go1)

data_res = data_res[,c(1,2,4,3)]
data_res = data_res[data_res$type == 'size',]
data_res$type = NULL

x = data_res[data_res$Responses == "unswitch",]$data_y
y = data_res[data_res$Responses == "switch",]$data_y
res = t.test(x,y,var.equal=T,paired=T)

tVal = round(res[["statistic"]][["t"]],3)
pVal = round(res[["p.value"]],3)

cohen_d = cohen.d(x,y,paired=TRUE, within=TRUE)
cohen_d = round(abs(cohen_d[["estimate"]]),3)

bayesF = ttestBF(x = x, y = y, paired=TRUE)
bayesF = round(exp(bayesF@bayesFactor[["bf"]]),3)

#### Figure 4B(tertile) ####
# data_tertile=fromJSON(file="./data/Exp2/data_tertile20210610.json")
# data_tertile=fromJSON(file="../[Python]PreProcessing/Exp2/data/data_tertile20210610.json")
if(mmFlg){
  data = fromJSON(file="../[Python]PreProcessing/Exp2/data/data_tertile_mm.json")
}else if(auFlg){
  data = fromJSON(file="../[Python]PreProcessing/Exp2/data/data_tertile_au.json")
}else{
  data = fromJSON(file="../[Python]PreProcessing/Exp2/data/data_tertile20210610.json")
}

data_tertile_e2 = data.frame(
  sub = data$sub,
  Tertile = data$tertile,
  responses_sorted = data$responses_sorted,
  data_y = data$responses_sorted
)
data_tertile_e2 = aggregate( . ~ sub*Tertile, data = data_tertile_e2, FUN = "mean")

data_tertile_e2$sub = subName[data_tertile_e2$sub]

x = as.numeric(data_tertile_e2$Tertile)
y = data_tertile_e2$data_y
f <- y ~  a*x + b
model1 <- nls(f, start = c(a = 0, b = 0))

f <- y ~  a*x^2 + b*x + c
model2 <- nls(f, start = c(a = 0, b = 0, c=0))

if (AIC(model1) > AIC(model2)){
  model = model2
}else{
  model = model1
}
df <- data.frame(x = seq(1, 5, length = 10))
data_curve = data.frame(
  Tertile = df$x,
  yy = predict(model, df)
)

tertile_e2_tVal = round(summary(model)[["coefficients"]]['b',]['t value'],4)
tertile_e2_pVal = round(summary(model)[["coefficients"]]['b',]['Pr(>|t|)'],4)

tertile_e2_BF = regressionBF(data_y ~ Tertile, data = data_tertile_e2)
tertile_e2_BF = round(exp(tertile_e2_BF@bayesFactor[["bf"]]),3)

# lm_model_e2 = lm(data = data_tertile_e2, data_y ~ Tertile)
data_tertile_e2 = aggregate( . ~ sub*Tertile, data = data_tertile_e2, FUN = "mean")
# r_e2 = cor.test(data_tertile_e2$Tertile, data_tertile_e2$responses_sorted)
r_e2 = cor.test(x,y)

f = data.frame(
  sub = as.character(unique(data_tertile_e2$sub)),
  x = data_tertile_e2$Tertile,
  y = data_tertile_e2$responses_sorted
)

#### Figure 5 ####
load('./data/dataset_figure5_2.rda')

# ind_data = ind_data[ind_data$events == 'dilation',]
# data_ave = aggregate( data_y ~ sub*numOfSwitch*exp, data = ind_data, FUN = "mean")

numOfSub = length(unique(data_anovaPD$sub))
f = data.frame(
  sub = as.character(unique(data_anovaPD$sub)),
  y = matrix(data_anovaPD$data_y,nrow = numOfSub)
)

colnames(f) <- c("subject",c('Exp1-unswitch','Exp1-switch','Exp2-unswitch','Exp2-switch'))
write.csv(f, "../[JASP]Bayesian/data_anovaPD.csv",row.names=FALSE)

anovakun(data_anovaPD,"sAB",long=T, peta=T,gg=T)
event_PD_table = forDrawingSigANOVA

data_anovaPD$numOfSwitch = factor(data_anovaPD$numOfSwitch,levels = unique(data_anovaPD$numOfSwitch))
data_anovaPD$exp = factor(data_anovaPD$exp,levels = unique(data_anovaPD$exp))
data_anovaPD$sub = factor(data_anovaPD$sub,levels = unique(data_anovaPD$sub))

data_anovaPD_BF = anovaBF(data_y ~ numOfSwitch*exp + sub, data=data_anovaPD, whichRandom = "sub")
data_anovaPD_BF = round(exp(data_anovaPD_BF@bayesFactor[["bf"]]),3)

anovakun(data_anovaPC,"sAB",long=T, peta=T,gg=T)
event_PC_table = forDrawingSigANOVA

data_anovaPC$numOfSwitch = factor(data_anovaPC$numOfSwitch,levels = unique(data_anovaPC$numOfSwitch))
data_anovaPC$exp = factor(data_anovaPC$exp,levels = unique(data_anovaPC$exp))
data_anovaPC$sub = factor(data_anovaPC$sub,levels = unique(data_anovaPC$sub))

data_anovaPC_BF = anovaBF(data_y ~ numOfSwitch*exp + sub, data=data_anovaPC, whichRandom = "sub")
data_anovaPC_BF = round(exp(data_anovaPC_BF@bayesFactor[["bf"]]),3)

```


## Experiment 2
### Behavioral response
The average number of ‘yes’ and ‘no’ trials (presence and absence of perceptual alternations, respectively) were 
`r ave_numOfTrial[ave_numOfTrial$Responses == 0,]$numOfTrial` $\pm$ `r sd_numOfTrial[sd_numOfTrial$Responses == '0',]$numOfTrial` and 
`r ave_numOfTrial[ave_numOfTrial$Responses == '1',]$numOfTrial` $\pm$ `r sd_numOfTrial[sd_numOfTrial$Responses == '1',]$numOfTrial`, respectively 
($t$(1,`r numOfTrial_e2_table[["parameter"]][["df"]]`) = `r round(numOfTrial_e2_table[["statistic"]][["t"]],3)`, $p$ = `r round(numOfTrial_e2_table[["p.value"]],3)`, Cohen’s  $d_z$ = `r numOfTrial_e2_cohen_d`). 
Again, there was no statistical differences between 'yes' or 'no' trial and the observation time 
($t$(1,`r jitter_table_e2[["parameter"]][["df"]]`) = `r round(jitter_table_e2[["statistic"]][["t"]],3)`, $p$ = `r round(jitter_table_e2[["p.value"]],3)`, Cohen’s  $d_z$ = `r jitter_cohen_d_e2`). 

RT in the answer ‘yes’ (`r rt_ave[rt_ave$Responses == 0,]$RT` $\pm$ `r rt_sd[rt_sd$Responses == 0,]$RT` s) was significantly faster than ‘no’ (`r rt_ave[rt_ave$Responses == 0,]$RT` $\pm$ `r rt_sd[rt_sd$Responses == 0,]$RT` s)  
($t$(1,`r RT_table[["parameter"]][["df"]]`) = `r round(RT_table[["statistic"]][["t"]],3)`, $p$ = `r round(RT_table[["p.value"]],3)`, Cohen’s  $d_z$ = `r RT_cohen_d`). This could be because, the participant in the ‘yes’ trial would have been ready to respond as soon as the first occurrence of a perceptual alternation before the response cue was presented, whereas the participant had to wait until the cue to say ‘no’. It is important to note, therefore, that the perceptual load and/or mental effort is expected to be lower in the ‘yes’ trials than in the ‘no’ ones. In Experiment 1, the participant had to keep counting and memorizing the number of alternations throughout the observation period, possibly leading to increasing perceptual load in the trials with increasing number of alternations. Thus, there was a concern that the association between the pupil size and number of perceptual alternations observed in Experiment 1 reflected such a perceptual load, rather than the processes involved in perceptual switching. The current yes/no paradigm serves as a control test to evaluate the confounding effect of this task-related perceptual load. 

### Baseline pupil size
Figure 3A shows the grand-averaged time-course of baseline pupil changes parameterized by alternation cases (yes or no). Consistent with Experiment 1, a paired t-test for averaged changes in baseline pupil size from -1000 ms to the response cue onset for each answer (i.e., the presence or absence of perceptual alternation) showed that the baseline pupil size in the presence of a perceptual alternation was significantly larger than in the absence of perceptual alternation ($t$(1, `r numOfSub_e2-1`) = `r round(tVal[1],3)`, $p$ = `r round(pVal[1],3)`, Cohen’s  $d_z$ = `r cohen_d`, $BF_{10}$ =  `r bayesF`).


Following the same analysis procedure as in Experiment 1, we segregated the trials into five bins based on the ranked order of the normalized baseline pupil size. For each participant, we normalized the probability of perceptual alternation by z-scores and averaged them in each pupil size bin (Fig. 3B). The model fitted by a simple regression showed the significance ($y$ = `r round(summary(model)[['coefficients']]['a',]['Estimate'],3)`$x$ + `r round(summary(model)[['coefficients']]['b',]['Estimate'],3)`, $R$ = `r round(r_e2[['estimate']][['cor']],3)`, $t$ = `r tertile_e2_tVal`, $p$ = `r tertile_e2_pVal`).



<!-- ### Correlation heatmap -->
<!-- As a further analysis, we calculated the correlation between the number of switch and several eye-metrics (__Figure 2C__); besides the significant correlation with baseline pupil size ($R$ = `r melted_data[melted_data$L1 == 'numOfSwitch' & melted_data$L2 == 'Baseline',]$value`, $p$ = `r melted_data[melted_data$L1 == 'numOfSwitch' & melted_data$L2 == 'Baseline',]$p.value`), besides the significant positive correlation between the number of switch and baseline pupil size ($R$ = `r melted_data[melted_data$L1 == 'Transient' & melted_data$L2 == 'Baseline',]$value`, $p$ = `r melted_data[melted_data$L1 == 'Transient' & melted_data$L2 == 'Baseline',]$p.value`). Since the pupil change is mediated by the dilater/sphincter muscle in the iris, this might be interpreted as a trade-off mechanism of the iris musculature or activation of ANS  __(Joshi et al., 2016)__. -->
## Transient Pupil Dilation/Constriction (PD/PC)
To assess the relationship between perceptual alternations and transient pupil change reported previously (Einhäuser et al., 2008; Grenzebach et al., 2021; Turi et al., 2018), we calculated the rate of PD/PC events (see Methods). Figure 4A shows the occurrence of PD/PC events for each trial across all subjects, over a period of 2 s before the task response to 4 s after it. We averaged the number of PD events over a period of 4 s after the task response (Fig. 4B). To compare by the within-subject design, the participants who were not rejected in both Experiments 1 and 2 were examined in the following analysis. Two-way repeated measures ANOVAs on the averaged PD events with the response content and experiment as within-subject factors revealed that the average number of PD event was significantly larger in alternation trials than in no-alternation trials
($F$(`r round(event_PD_table[2,]['df.col'],2)`,`r round(event_PD_table[3,]['df.col'],2)`) = `r round(event_PD_table[2,]['f.col'],3)`, 
$p$ = `r round(event_PD_table[2,]['p.col'],3)`, $\eta^2_p$ = `r round(event_PD_table[2,]['p.eta^2'],3)`, $BF_{10}$ =  `r  data_anovaPD_BF[1]`), 
consistent with the previous studies (Einhäuser et al., 2008; Grenzebach et al., 2021). 
The number of PD events was larger in Experiment 1 than in 2 
($F$(`r round(event_PD_table[4,]['df.col'],2)`,`r round(event_PD_table[5,]['df.col'],2)`) = `r round(event_PD_table[4,]['f.col'],3)`, 
$p$ = `r round(event_PD_table[4,]['p.col'],3)`, $\eta^2_p$ = `r round(event_PD_table[4,]['p.eta^2'],3)`, $BF_{10}$ =  `r  data_anovaPD_BF[2]`), 
which could be explained by the higher task demand in Experiment 1, as the LC-NE system reflects a broad range of cognitive processes. There was no interaction between the response content and experiment 
($F$(`r round(event_PD_table[6,]['df.col'],2)`,`r round(event_PD_table[7,]['df.col'],2)`) = `r round(event_PD_table[6,]['f.col'],3)`, 
$p$ = `r round(event_PD_table[6,]['p.col'],3)`, $\eta^2_p$ = `r round(event_PD_table[6,]['p.eta^2'],3)`, $BF_{10}$ =  `r  round(data_anovaPD_BF[4]/data_anovaPD_BF[3],3)`). 
Two-way repeated measures ANOVAs on the averaged PC showed that there were no significant main effect and interaction 
($F$(`r round(event_PC_table[2,]['df.col'],3)`,`r round(event_PC_table[3,]['df.col'],3)`) = `r round(event_PC_table[2,]['f.col'],3)`, 
$p$ = `r round(event_PC_table[2,]['p.col'],3)`, $\eta^2_p$ = `r round(event_PC_table[2,]['p.eta^2'],3)`, $BF_{10}$ =  `r  data_anovaPC_BF[1]`); 
$F$(`r round(event_PC_table[4,]['df.col'],3)`,`r round(event_PC_table[5,]['df.col'],3)`) = `r round(event_PC_table[4,]['f.col'],3)`, 
$p$ = `r round(event_PC_table[4,]['p.col'],3)`, $\eta^2_p$ = `r round(event_PC_table[4,]['p.eta^2'],3)`, $BF_{10}$ =  `r  data_anovaPC_BF[2]`; 
$F$(`r round(event_PC_table[6,]['df.col'],3)`,`r round(event_PC_table[7,]['df.col'],3)`) = `r round(event_PC_table[6,]['f.col'],3)`, 
$p$ = `r round(event_PC_table[6,]['p.col'],3)`, $\eta^2_p$ = `r round(event_PC_table[6,]['p.eta^2'],3)`, $BF_{10}$ =  `r  round(data_anovaPC_BF[4]/data_anovaPC_BF[3],3)`, respectively).

## Cross-correlation function
The baseline pupil size up to six trials ahead of the behavioral response and after the response until three trials was positively correlated to the number of perceptual alternations (Fig. 5A). 
As the intertrial interval was jittered in 5–7 s and the baseline pupil size was inherently defined as 5–7 s before the response in the 0-lag condition, the result indicates that the baseline pupil size predicted perceptual alternation at least 35 s (= 5 s x 6 trials + 5 s) before the behavioral response and that the overall correspondence between pupil size and perceptual alternation was over a sustained time window of 45 s (5 s x 9 trials) at minimum and of 54 s (6 s x 9 trials) at the averaged interval. 
<!-- Figure 5B shows that the averaged maximum cross-correlation coefficient (found for each subject) in the raw condition was significantly higher than in the shuffled condition ($t$(1, `r numOfsub-1`) = `r round(CCF_max[["statistic"]][["t"]],3)`, $p$ = `r round(CCF_max[["p.value"]], 3)`, Cohen’s  $d_z$ = `r CCF_max_cohen_d`, $BF_{10}$ =  `r round(CCF_max_bayesF, 3)`).  -->





<!-- Each trial lasted for 5-7 s, segregated by a response cue to ask for behavioral responses. In the cross-correlation analysis, we aligned the timing of behavioral response and pupil size throughout the experimental session (see Method). Cross-correlation functions between the changes in baseline pupil size and the number of perceptual switches were calculated in every single subject. We found that the number of switches was positively correlated with the pupil size up to 5 trials (nearly 25 to 35 s) ahead of the response time (Figure 3A). The maximum cross-correlation coefficient in raw cross-correlation was significantly higher than in shuffled (__Figure 3A__). -->
<!-- The maximum cross-correlation coefficient in raw cross-correlation was significantly higher than in shuffled  ($t$(1, `r numOfsub-1`) = `r round(CCF_max[["statistic"]][["t"]],3)`, $p$ = `r round(CCF_max[["p.value"]], 3)`, Cohen’s  $d_z$ = `r CCF_max_cohen_d`, $BF_{10}$ =  `r CCF_max_bayesF`). Furthermore, the averaged lag at peak correlation was significantly shifted to negative ($t$(1, `r numOfsub-1`) = `r round(CCF_lag[["statistic"]][["t"]],3)`, $p$ = `r round(CCF_lag[["p.value"]], 3)`, $BF_{10}$ =  `r CCF_lag_bayesF`) (__Figure 3C__). -->

<!-- ### Liner mixed modeling and ROC curve -->
<!-- In this section, we focused on (a) how long the baseline pupil size effect on the number of perceptual switches at a certain trial lasts, (b) variability within participants, (c) classification accuracy. To address these, we calculated beta weights using a linear mixed-model (LMM) (Figure 3D) regarded the number of perceptual switches as dependent variable, the baseline pupil size as fixed effect and participants as random effect. For each trial, LMM was performed repeatedly changing the baseline pupil size data adopted from -20 to 20 trials corresponding to the answer at a certain trial. In consistent with the cross-correlation analysis, the LMM revealed a significant positive correlation between the number of switch and the baseline pupil size with not larger individual differences at least from -7 to 0 trial (i.e., 30s). Furthermore, the detection performance was quantified by the area under the curve (AUC) using receiver operating characteristic (ROC) curves (Figure 3E). Wilcoxon rank-sum test showed that the AUC from -5 to 5 trial were significantly greater than chance level (i.e., 0.5). -->

<!-- $CCF(\tau) = \frac{Cov(P(t),N_{switch}(t+\tau))}{\sigma_{P(t)}\sigma_{N_{switch}(t+\tau)}}$  -->

<!-- ### PC events -->
<!-- Main effect on alternation -->
<!-- Main effect on experiments  -->
<!-- Interaction -->
<!-- ($F$(1,`r round(event_PC_table[1,]['df.col'],3)`) = `r round(event_PC_table[2,]['f.col'],3)`, $p$ = `r round(event_PC_table[2,]['p.col'],3)`, $\eta^2_p$ = `r round(event_PC_table[2,]['p.eta^2'],3)`; $F$(1,`r round(event_PC_table[3,]['df.col'],3)`) = `r round(event_PC_table[4,]['f.col'],3)`, $p$ = `r round(event_PC_table[4,]['p.col'],3)`, $\eta^2_p$ = `r round(event_PC_table[4,]['p.eta^2'],3)`; $F$(1,`r round(event_PC_table[5,]['df.col'],3)`) = `r round(event_PC_table[6,]['f.col'],3)`, $p$ = `r round(event_PC_table[6,]['p.col'],3)`, $\eta^2_p$ = `r round(event_PC_table[6,]['p.eta^2'],3)`). -->

### Statistical analysis
A one-way repeated-measures analysis of variance (ANOVA) was performed using the baseline pupil size and the number of switches in Experiment 1 and presence vs. absence of perceptual switch in Experiment 2 as within-subject factors. In pairwise comparisons of the main effects, uncorrected p values are reported with the Bonferroni-corrected alpha level.
 Effect sizes were given as partial $\eta^2$; $\eta^2_p$ for ANOVA and as Cohen's $d_z$ for $t$-tests __(Cohen, 1998, p. 48)__. 
<!-- $Cohen's\:d_z = \frac{\rvert\mu_1 - \mu_2\rvert}{s}$ -->
<!-- $,s = \sqrt\frac{n_1s_1^2+n_2s_2^2}{n_1+n_2}$ -->
<!-- For pair-wise comparison, the level of statistical significance was set to $p$ < 0.05 for all analyses. 
-->
<!-- where $mu$ as the mean difference between their responses in each condition and $P$ as the standard deviation. -->
To quantify the evidence in the data, we performed Bayesian one-sample t-tests using the BayesFactor package (v0.9.12-4.2) __(Morey, 2019)__ for the R software (Version 3.6.3) __(R Core Team, 2020)__. We reported Bayesian Factor (BF) estimating the relative weight of the evidence in favor of $H_1$ over $H_0$ as $BF_{10}$. Greenhouse–Geisser corrections were performed when the results of Mauchly’s sphericity test were significant.

In the analysis of pupil response bins, we fitted the following two models to assess  whether the behavioral variability ($Y$) can be explained by a second-order polynomials or monotonic fitting.


Model 1 : $Y = \beta_0 + \beta_1P$

Model 2 : $Y = \beta_0 + \beta_1P + \beta_2P^2$


where $\beta$ as regression coefficients and $P$ as the baseline pupil response bins. The models were quantified using the Akaike information criterion (AIC), which specifies the evidence of goodness of fit for a model.

<!-- In Experiment 1, the value of Pearson’s correlation was calculated at a lag between the pupil size and the normalized number of switches using $z$-score in cross-correlation. $p$ values for Pearson’s correlation were corrected by FDR for multiple comparisons using the Benjamini and Hochberg method __(Benjamini and Hochberg, 1995)__. -->


<!-- We used a linear mixed-effects modeling (LMM) with participant as a random effect to calculate the correlation between the individuals' baseline pupil size ($\vec{P}_{n}$) and the number of perceptual switch ($\vec{S_n}$) in Experiment 1 as follwing model, using the lme4 packages __(Bates et al., 2015)__. -->

<!-- $[S_i]_{n+t},(-t < i < n) \sim \alpha_t + \beta_t * [P_i]_{n+t},(0 < i < n+t) + N(0,\sigma_{participant}),(t \leq 0)$ -->

<!-- $[S_i]_{n+t},(0 < i < n-t) \sim \alpha_t + \beta_t * [P_i]_{n+t},(t < i < n ) + N(0,\sigma_{participant}),(t > 0)$ -->

<!-- where $\beta$ as beta weight and $n$ as indivisuals' total number of trials. The analyzed period was $-20 \leq t \leq 20$ -->


<!-- ### Pupil dilation/constriction events analysis -->
<!-- Pupil slope was computed using second order central differences after applying 1 s window of smoothing filter within a trial. Pupil dilation and constriction events were defined as maximum positive and negative values of the slope, separeted by $\leq$ 300 ms (__Joshi et al., 2016; Zhao et al., 2019__). The analysis interval was from 2 s before to 5 s after the task response. For each trial, the number of dilation events were summed and then all averaged between switch and unswitch condition in each participant. -->